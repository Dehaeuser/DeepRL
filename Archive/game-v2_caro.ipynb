{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd69a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from vocab import Vocabulary\n",
    "from env2 import ConceptData\n",
    "from create_data import addFile\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import create_data\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "import agents\n",
    "import sender\n",
    "from agents import find_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13450dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OPTIONS = 10\n",
    "NUM_DISTRACTORS = 9\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 44\n",
    "EMBED_DIM = 44\n",
    "VOCAB_SIZE = 99\n",
    "MAX_LEN = 10\n",
    "NUM_EPOCHS = 1\n",
    "TRAINING = True\n",
    "sender_entropy_coeff = 0.015 #wie bei Ossenkopf\n",
    "receiver_entropy_coeff = 0.0 # wie bei Ossenkopf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace1e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = Vocabulary()\n",
    "\n",
    "def addFile(name):\n",
    "    file_name = name + \"_structured_final.xml\"\n",
    "    file = minidom.parse(os.path.join(os.path.join('visa_dataset', 'UK'), file_name))\n",
    "    concepts = file.getElementsByTagName('concept')\n",
    "\n",
    "    for concept in concepts:\n",
    "        vocabulary.addConcept(concept)\n",
    "\n",
    "\n",
    "addFile(\"ANIMALS\")\n",
    "addFile(\"APPLIANCES\")\n",
    "addFile(\"ARTEFACTS\")\n",
    "addFile(\"CLOTHING\")\n",
    "addFile(\"CONTAINER\")\n",
    "addFile(\"DEVICE\")\n",
    "addFile(\"FOOD\")\n",
    "addFile(\"HOME\")\n",
    "addFile(\"INSTRUMENTS\")\n",
    "addFile(\"MATERIAL\")\n",
    "addFile(\"PLANTS\")\n",
    "addFile(\"STRUCTURES\")\n",
    "addFile(\"TOOLS\")\n",
    "addFile(\"TOYS\")\n",
    "addFile(\"VEHICLES\")\n",
    "addFile(\"WEAPONS\")\n",
    "\n",
    "for concept in vocabulary.concept_list:\n",
    "    vocabulary.parseConcept(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378cbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisiere die Agents\n",
    "speakerEncoder = sender.Sender(num_options=NUM_OPTIONS, batch_size=BATCH_SIZE)\n",
    "speakerEncoderOnlyTarget = sender.SenderOnlyTarget(batch_size=BATCH_SIZE)\n",
    "speakerDecoder = sender.Sender_LSTM(embed_dim=EMBED_DIM,\n",
    "                                    num_cells=1,\n",
    "                                    hidden_size=1, \n",
    "                                    max_len=MAX_LEN)\n",
    "receiverEncoder = agents.Receiver(num_options = NUM_OPTIONS)\n",
    "receiverDecoder = agents.Receiver_LSTM(agent=receiverEncoder, \n",
    "                                       vocab_size=VOCAB_SIZE,\n",
    "                                       embed_dim=EMBED_DIM, \n",
    "                                       hidden_size=HIDDEN_SIZE)\n",
    "guesser = agents.AuxillaryNetwork(hidden_size=HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588e1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the losses\n",
    "\n",
    "#loss of guessing the correct target\n",
    "def loss(input_concepts, receiver_output, targets):\n",
    "    \"\"\"\n",
    "    receiver_output ist was von receiver_sampling zur√ºckgegeben wird\n",
    "    LABELS PRINTEN IN OSSSENKOPF NOTEBOOK\n",
    "    \"\"\"\n",
    "    guesses = []\n",
    "    \n",
    "    for i in range(len(receiver_output)):\n",
    "        guesses.append(input_concepts[i][receiver_output[i]])\n",
    "            \n",
    "    guesses = tf.constant(guesses)\n",
    "    targets = tf.constant(targets)\n",
    "    acc = np.sum(guesses == targets) / guesses.shape[0]\n",
    "    \n",
    "    return -acc\n",
    "\n",
    "#auxiliary loss to promote empathy\n",
    "def auxiliary_loss(receiver_thoughts, \n",
    "                  # _message, _receiver_input, \n",
    "                   guesser_output, \n",
    "                   #_labels,\n",
    "                   weight=0.2):\n",
    "    mae = tf.keras.losses.MeanAbsoluteError(reduction = 'none')\n",
    "    loss = mae(receiver_thoughts, guesser_output)\n",
    "    loss *= weight\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e91871",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_guesser = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n",
    "optim_receiver = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n",
    "optim_sender = tf.keras.optimizers.Adam(learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d6b00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape sender input:\n",
      "(32, 10, 1, 595)\n",
      "32\n",
      "input-shape:\n",
      "(32, 10, 44)\n",
      "10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['receiver_lstm/receiver/encoder/dense_3/kernel:0', 'receiver_lstm/receiver/encoder/dense_3/bias:0', 'receiver_lstm/embedding/embeddings:0', 'receiver_lstm/lstm_1/lstm_cell_1/kernel:0', 'receiver_lstm/lstm_1/lstm_cell_1/recurrent_kernel:0', 'receiver_lstm/lstm_1/lstm_cell_1/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cb70138747d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mreceiver_gradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreceiverDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m#print(receiver_gradients)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0moptim_receiver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreceiver_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreceiverDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ReAlly\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \"\"\"\n\u001b[1;32m--> 598\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ReAlly\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[0;32m     79\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['receiver_lstm/receiver/encoder/dense_3/kernel:0', 'receiver_lstm/receiver/encoder/dense_3/bias:0', 'receiver_lstm/embedding/embeddings:0', 'receiver_lstm/lstm_1/lstm_cell_1/kernel:0', 'receiver_lstm/lstm_1/lstm_cell_1/recurrent_kernel:0', 'receiver_lstm/lstm_1/lstm_cell_1/bias:0']."
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    # 1. data holen, labels?\n",
    "    ####Fehlt hier noch\n",
    "    data = ConceptData(voc=vocabulary, num_distractors=NUM_DISTRACTORS, batch_size=BATCH_SIZE)\n",
    "    input_concepts, sender_input, targets, receiver_input = data.getInput()\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        ##2. durch agents laufen lassen\n",
    "        output_sender = speakerEncoder(sender_input)\n",
    "        message, log_prob_s, entropy_s , prev_hidden = speakerDecoder(output_sender)\n",
    "\n",
    "        #einer- Dimension rausbekommen\n",
    "        receiver_input = tf.squeeze(tf.convert_to_tensor(receiver_input))\n",
    "        #richtige Reihenfolge der Dimensionen\n",
    "        receiver_input2 = tf.transpose(receiver_input, [1,0,2])\n",
    "        receiver_input2 = receiver_input2.numpy()\n",
    "\n",
    "        sample, log_prob_r, entropy_r, last_hidden = receiverDecoder(message=message, \n",
    "                                                                     batch_size=BATCH_SIZE, \n",
    "                                                                     max_len=MAX_LEN,\n",
    "                                                                     receiver_input=receiver_input2)\n",
    "        guesser_output = guesser(prev_hidden)\n",
    "\n",
    "        output_receiver = []\n",
    "        for j in range(BATCH_SIZE):\n",
    "            output_receiver.append(input_concepts[i][sample[i]])\n",
    "\n",
    "\n",
    "        # 3. Loss berechnen\n",
    "        auxLoss = auxiliary_loss(last_hidden, guesser_output)\n",
    "        # labels fehlen noch\n",
    "        loss = loss(input_concepts=input_concepts, \n",
    "                    receiver_output=sample, \n",
    "                    targets=targets)\n",
    "\n",
    "        # compute effective entropy and log_prob of output before and including eos\n",
    "        message_lengths = find_lengths(message)\n",
    "\n",
    "        effective_entropy_s = tf.zeros_like(entropy_r)\n",
    "        effective_entropy_s = np.array(effective_entropy_s)\n",
    "        effective_log_prob_s = tf.zeros_like(log_prob_r)\n",
    "        effective_log_prob_s = np.array(effective_log_prob_s)\n",
    "\n",
    "        #print(message.shape)\n",
    "\n",
    "        for k in range(message.shape[0]):\n",
    "            for l in range(message.shape[1]):\n",
    "                not_eosed = float(k < message_lengths[k])\n",
    "                effective_entropy_s[k] += entropy_s[k, l] * not_eosed\n",
    "                effective_log_prob_s[k] += log_prob_s[k, l] * not_eosed\n",
    "\n",
    "        effective_entropy_s = tf.convert_to_tensor(effective_entropy_s)\n",
    "        effective_log_prob_s = tf.convert_to_tensor(effective_log_prob_s)\n",
    "\n",
    "        weighted_entropy = entropy_r * receiver_entropy_coeff + sender_entropy_coeff * effective_entropy_s\n",
    "        log_prob = log_prob_r + effective_log_prob_s\n",
    "        loss = tf.math.reduce_mean(loss * log_prob) - weighted_entropy\n",
    "\n",
    "        #optimization\n",
    "        receiver_gradients = tape.gradient(loss, receiverDecoder.trainable_variables)\n",
    "        guesser_gradients = tape.gradient(auxLoss, guesser.trainable_variables)\n",
    "        \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # muss der eine Receiver im Konstruktor vom anderem erstellt werden damit das funktioniert?\n",
    "        receiver_gradients = tape.gradient(loss, receiverDecoder.trainable_variables)\n",
    "        #print(receiver_gradients)\n",
    "    optim_receiver.apply_gradients(zip(receiver_gradients, receiverDecoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        guesser_gradients = tape.gradient(auxLoss, guesser.trainable_variables)\n",
    "        #print(guesser_gradients)\n",
    "    optim_guesser.apply_gradients(zip(guesser_gradients, guesser.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        senderLSTM_gradients = tape.gradient(loss, speakerDecoder.trainable)\n",
    "        senderEncoder_gradients = guesser_gradients + senderLSTM_gradients\n",
    "    optim_sender.apply_gradients(zip(senderLSTM_gradients, speakerDecoder.trainable_variables))\n",
    "    optim_sender.apply_gradients(zip(senderEncoder_gradients, senderEncoder.trainable_variables))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #testing <-> trainifehlt noch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe55859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
