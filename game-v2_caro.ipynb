{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b1cec7-61ed-4066-9b89-28f7d3d25c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from vocab import Vocabulary\n",
    "from env import ConceptData\n",
    "from create_data import addFile\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import create_data\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "from vocab import Vocabulary\n",
    "from env import ConceptData\n",
    "from create_data import addFile\n",
    "import agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dff81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OPTIONS = 10\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 99\n",
    "NUM_DISTRACTORS = 9\n",
    "MAX_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bad41f30-054a-4293-949a-c997256a8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32\n",
    "hidden_size = 44\n",
    "num_distractors = 7\n",
    "#ist an sich vorgegeben und 597 oder so\n",
    "num_categories = 58\n",
    "vocab_size = 99\n",
    "embed_dim = 50\n",
    "max_len = 5\n",
    "num_epochs = 12\n",
    "sender_entropy_coeff = 0.015 #wie bei Ossenkopf\n",
    "receiver_entropy_coeff = 0.0 # wie bei Ossenkopf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f224a1c4-05bd-4850-bc4a-1ea13263b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_concepts:\n",
      "[<DOM Element: concept at 0x202fc0eb670>, <DOM Element: concept at 0x202fb311a60>, <DOM Element: concept at 0x202fbfab9d0>, <DOM Element: concept at 0x202fc1448b0>, <DOM Element: concept at 0x202fbf9b670>, <DOM Element: concept at 0x202fc0eb8b0>, <DOM Element: concept at 0x202fb87b040>, <DOM Element: concept at 0x202fc08ce50>, <DOM Element: concept at 0x202fbf865e0>, <DOM Element: concept at 0x202fc0eb4c0>]\n",
      "target_concept:\n",
      "<DOM Element: concept at 0x202fc08ce50>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary = Vocabulary()\n",
    "\n",
    "def addFile(name):\n",
    "    file_name = name + \"_structured_final.xml\"\n",
    "    file = minidom.parse(os.path.join(os.path.join('visa_dataset', 'UK'), file_name))\n",
    "    concepts = file.getElementsByTagName('concept')\n",
    "\n",
    "    for concept in concepts:\n",
    "        vocabulary.addConcept(concept)\n",
    "\n",
    "\n",
    "addFile(\"ANIMALS\")\n",
    "addFile(\"APPLIANCES\")\n",
    "addFile(\"ARTEFACTS\")\n",
    "addFile(\"CLOTHING\")\n",
    "addFile(\"CONTAINER\")\n",
    "addFile(\"DEVICE\")\n",
    "addFile(\"FOOD\")\n",
    "addFile(\"HOME\")\n",
    "addFile(\"INSTRUMENTS\")\n",
    "addFile(\"MATERIAL\")\n",
    "addFile(\"PLANTS\")\n",
    "addFile(\"STRUCTURES\")\n",
    "addFile(\"TOOLS\")\n",
    "addFile(\"TOYS\")\n",
    "addFile(\"VEHICLES\")\n",
    "addFile(\"WEAPONS\")\n",
    "\n",
    "for concept in vocabulary.concept_list:\n",
    "    vocabulary.parseConcept(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9b8c98-d3e8-4f03-a1df-7dd739638fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff03c97e-b604-4682-a6db-753df72a5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisiere die Agents\n",
    "speakerEncoder = sender.Sender(num_options=NUM_OPTIONS, batch_size=BATCH_SIZE)\n",
    "speakerEncoderOnlyTarget = sender.SenderOnlyTarget(batch_size=BATCH_SIZE)\n",
    "speakerDecoder = agents.Sender_LSTM(embed_dim=embed_dim, num_cells=1, hidden_size=1, max_len=MAX_LEN)\n",
    "receiverEncoder = agents.Receiver(num_distractors)\n",
    "receiverDecoder = agents.Receiver_LSTM(receiverEncoder, vocab_size, embed_dim, hidden_size)\n",
    "guesser = agents.AuxillaryNetwork(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6fd4ce-3608-4b63-9b00-22183bbb1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_hidden kommt von sender; müssen wir hier faken\n",
    "#prev_hidden = tf.random.normal([32,44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bc405d-46b2-45c7-bf88-8e737c6cd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guesser_output = guesser(prev_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "336f56cf-7880-4115-9c22-5cdc70d0c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the losses\n",
    "#loss um prediction mit Realität zu vergleichen\n",
    "#für was brauchen wir die underscore variablen?\n",
    "\n",
    "# TODO:\n",
    "# loss anpassen!\n",
    "\n",
    "def loss(_sender_input, _message, _receiver_input, receiver_output, labels):\n",
    "    \"\"\"\n",
    "    receiver_output ist was von receiver_sampling zurückgegeben wird\n",
    "    LABELS PRINTEN IN OSSSENKOPF NOTEBOOK\n",
    "    \"\"\"\n",
    "    acc = (labels.t() == receiver_output).float() - (labels.t() != receiver_output).float()\n",
    "    return -acc, {'acc': (acc.mean().item()+1)/2}\n",
    "\n",
    "#loss um prediction mit Realität zu vergleichen\n",
    "def auxiliary_loss(receiver_thoughts, \n",
    "                  # _message, _receiver_input, \n",
    "                   guesser_output, \n",
    "                   #_labels,\n",
    "                   weight=0.2):\n",
    "    mae = tf.keras.losses.MeanAbsoluteError(reduction = 'none')\n",
    "    loss = mae(receiver_thoughts, guesser_output,)\n",
    "    loss *= weight\n",
    "    return loss, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be22afbf-c5fb-4a29-b5fd-be61aff07255",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_guesser = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n",
    "optim_receiver = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n",
    "optim_sender = tf.keras.optimizers.Adam(learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b509be3b-ed51-41e7-aefc-33cb108cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epochs):\n",
    "    # 1. data holen, labels?\n",
    "    ####Fehlt hier noch\n",
    "    data = ConceptData(voc=vocabulary, num_distractors=NUM_DISTRACTORS, batch_size=BATCH_SIZE)\n",
    "    sender_input, targets, receiver_input = data.getInput()\n",
    "    \n",
    "    \n",
    "    ##2. durch agents laufen lassen\n",
    "    output_sender = speakerEncoder(sender_input)\n",
    "    message, log_prob_s, entropy_s , prev_hidden = speakerDecoder(output_sender)\n",
    "    sample, log_prob_r, entropy_r, last_hidden = receiverDecoder(message, batch_size, max_len,vocab)\n",
    "    guesser_output = guesser(prev_hidden)\n",
    "        \n",
    "    \n",
    "    # 3. Loss berechnen\n",
    "    auxLoss = auxiliary_loss(last_hidden, guesser_output)\n",
    "    # labels fehlen noch\n",
    "    loss = loss(sample, labels)\n",
    "    ##policy_length_loss und das baseline Zeug miteinbringen?\n",
    "    weighted_entropy = entropy_r * receiver_entropy_coeff + sender_entropy_coeff # *entropy_r\n",
    "    log_prob = log_prob_r # + log_prob_s\n",
    "    loss = tf.math.reduce_mean(loss* log_prob) - weighted_entropy\n",
    "    \n",
    "    #optimization\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # muss der eine Receiver im Konstruktor vom anderem erstellt werden damit das funktioniert?\n",
    "        receiver_gradients = tape.gradient(loss, receiverDecoder.trainable_variables)\n",
    "        print(receiver_gradients)\n",
    "    optim_receiver.apply_gradients(zip(receiver_gradients, receiverDecoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        guesser_gradients = tape.gradient(auxLoss, guesser.trainable_variables)\n",
    "        print(guesser_gradients)\n",
    "    optim_guesser.apply_gradients(zip(guesser_gradients, guesser.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        senderLSTM_gradients = tape.gradient(loss, speakerDecoder.trainable)\n",
    "        senderEncoder_gradients = guesser_gradients + senderLSTM_gradients\n",
    "    optim_sender.apply_gradients(zip(senderLSTM_gradients, speakerDecoder.trainable_variables))\n",
    "    optim_sender.apply_gradients(zip(senderEncoder_gradients, senderEncoder.trainable_variables))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #testing <-> trainifehlt noch\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
